@&#MAIN-TITLE@&#Real-time pose estimation of rigid objects in heavily cluttered environments

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A method for pose estimation of multiple mutually occluding objects was proposed.


                        
                        
                           
                           Normal information reduces number of false positive detections.


                        
                        
                           
                           Combining surface normals and edge information with multiplication performs better for nearly planar objects.


                        
                        
                           
                           Iterative pose hypothesis selection improves detection rate for mutually occluded objects.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

object pose estimation

textureless surfaces

@&#ABSTRACT@&#


               
               
                  In this paper, we present a method for real-time pose estimation of rigid objects in heavily cluttered environments. At its core, the method relies on the template matching method proposed by Hinterstoisser et al., which is used to generate pose hypotheses. We improved the method by introducing a compensation for bias toward simple shapes and by changing the way modalities such as edges and surface normals are combined. Additionally, we incorporated surface normals obtained with photometric stereo that can produce a dense normal field at a very high level of detail. An iterative algorithm was employed to select the best pose hypotheses among the possible candidates provided by template matching. An evaluation of the pose estimation reliability and a comparison with the current state-of-the-art was performed on several synthetic and several real datasets. The results indicate that the proposed improvements to the similarity measure and the incorporation of surface normals obtained with photometric stereo significantly improve the pose estimation reliability.
               
            

@&#INTRODUCTION@&#

Visual inspection systems are often crucial for the detection of defects in manufactured products and for diagnosing problems in a manufacturing process. Currently, one characteristic of automated visual inspection systems is their specialization. With few exceptions, nearly all of the existing automated visual inspection systems have been designed to inspect a single object or a part of one whose position is highly constrained [1]. Positioning is usually achieved by a mechanical manipulation of the object, which can be expensive, space and/or time consuming, or simply impossible in some scenarios. A visual inspection system that could inspect arbitrarily positioned objects would bypass the need for mechanical manipulation of the inspected parts, thus reducing the cost of the system and increasing its flexibility. It would also enable inspection in scenarios that were previously thought unfeasible.

For visual inspection in mechanically unconstrained environments, the system needs to detect and localize the objects, before the inspection can take place; this can be challenging because of variable object appearance, changes in the environment, mechanical properties, and background clutter. On the other hand, the fact that most industrial lines produce millions of similar products provides several opportunities to exploit. For example, 3D models are in most cases readily available, and even if they are not, the fact that the assembly lines handle millions of objects, makes an acquisition of such a model economically justifiable.

In this paper, we present a method for real-time pose estimation in heavily cluttered environments. The method estimates the poses of multiple objects of the same type that are arbitrarily positioned in an image acquired by a single camera. We present three ideas that allow us to achieve a fast, reliable, and accurate operation: an improvement of the template matching algorithm proposed by Hinterstoisser et al. [2], a combining of the orientation of depth edges with the surface normal orientation obtained by photometric stereo, and an iterative procedure for selecting pose hypotheses.

The primary goal of this research was to develop a robust and reliable system for real-time pose estimation in cluttered environments that could be used for on-line analysis of objects in mechanically unconstrained environments (Fig. 1
                     ).

The paper is organized as follows. Brief overview of the related literature is presented in Section 2. Template matching is described in Section 3, followed by a description of hypotheses generation and selection in Section 4. An evaluation and the comparison to the existing state-of-the-art are in Section 5. Section 6 contains a discussion of the results, followed by the paper’s conclusion in Section 7.

@&#RELATED WORK@&#

A variety of approaches has been proposed for (textureless) pose estimation. Early attempts to estimate a 3D pose from a 2D image were made by Lowe [3], where a 3D wireframe model was matched to a 2D image, by establishing the correspondences between the distinctive lines on the model and an image. Correspondences were iteratively established by matching nearest lines on the model and an image. Similar approach with a modified optimization step was proposed by Dementhon and Davis [4] and subsequently by David and DeMenthon [5]. However, establishing correspondences by nearest neighbor search requires a starting pose close to the correct pose, therefore the applicability of the aforementioned methods is limited.

To circumvent this requirement, several descriptor-based methods have been proposed, which match the representative description [6,7] of the local regions on both the query and the reference image. For pose estimation, reference images are generated for all relevant views, thus the established correspondences determine both the position and the rotation of the object. Descriptor-based approaches demonstrate outstanding performance in several scenarios [8] but are generally restricted to textured objects. It is only recently that several attempts have been made to extract meaningful descriptors from textureless objects [9–12]. BOLD features [9] tackle textureless objects with a compact and distinct representation of groups of neighboring line segments aggregated over limited spatial supports. Damen et al. [12] combined an extraction of edgelet constellations with a library lookup based on rotation- and scale-invariant descriptors. Their distinguishing element was using path tracing for extracting edgelet constellations. Ferrari et al. [11] proposed a family of scale-invariant shape descriptors utilized in a shape-matching framework through a voting scheme in a Hough space.

Template matching methods, on the other hand, are usually based on exhaustive search and typically use a priori edge information for matching. Chamfer matching [13] was proposed decades ago for matching shape templates, and it remains the preferred method when simplicity is required. Nevertheless, it has a high computational complexity, which makes the naïve approach unfeasible for real-time applications. Borgefors [14] proposed a hierarchical Chamfer matching that significantly reduced the computational load. In Liu et al. [15,16], Chamfer matching was extended to include edge orientations and exhaustive search replaced with a 1D search along distinctive lines, leading to drastic improvements in both speed and accuracy. In Lampert et al. [17], exhaustive search was replaced by a branch and bound search strategy, significantly reducing the computational load. Choi and Christensen [18] combined the detection and tracking of textureless transparent objects within a particle-filtering framework, nonetheless still using Chamfer matching to find the set of starting states. Cai et al. [19] matched a compact representation of a shape in a sliding window with a library of reference representations. They extracted a compact representation at each position of a sliding window by finding the distance from uniformly positioned points in the window to their nearest edge point. All the aforementioned methods rely on binary edge images obtained by edge extraction methods [20–22].

Steger [23,24] proposed several similarity measures, for template matching, inherently robust against occlusion, clutter, and nonlinear illumination changes. One such measure was to sum the normalized dot product of the direction vectors of a transformed model and an image over all the points of the model; the measure returns a high score if all the direction vectors of the model and the query image align. To make the similarity measure truly illumination invariant, Steger discarded the magnitude of the orientation vectors and retained only the orientation difference. A comparable similarity measure was proposed by Hinterstoisser et al. [25] but with additional robustness to small translations and deformations. In addition, they extended the applicability of the method to arbitrary quantizable modalities and any combination thereof.

The proposed method is based on the template matching proposed by Hinterstoisser et al. [25], which can match any modalities that can be quantized. Template matching is used to evaluate the similarity measure for all the reference templates and to generate probability maps for all the reference templates. We use edge orientation combined with surface normal orientation obtained with photometric stereo. In this section, we first briefly describe the method as proposed by Hinterstoisser et al. and then propose several improvements to it.

Hinterstoisser et al. [25] proposed a similarity measure robust to small translations and deformations. The similarity measure 
                           E
                         can be formalized as:

                           
                              (1)
                              
                                 
                                    E
                                    
                                       (
                                       I
                                       ,
                                       T
                                       ,
                                       c
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          r
                                          ∈
                                          P
                                       
                                    
                                    
                                       max
                                       
                                          t
                                          ∈
                                          R
                                          (
                                          c
                                          +
                                          r
                                          )
                                       
                                    
                                    
                                       |
                                       cos
                                       
                                          (
                                          o
                                          r
                                          i
                                          
                                             (
                                             O
                                             ,
                                             r
                                             )
                                          
                                          −
                                          o
                                          r
                                          i
                                          
                                             (
                                             I
                                             ,
                                             t
                                             )
                                          
                                          )
                                       
                                       |
                                    
                                    ,
                                 
                              
                           
                        where 
                           
                              o
                              r
                              i
                              (
                              O
                              ,
                              r
                              )
                           
                         is the orientation in radians on reference image 
                           O
                         at location r and 
                           P
                         is a list of locations r to be considered in 
                           O
                        . Template 
                           T
                         is therefore defined as a pair 
                           
                              T
                              =
                              (
                              O
                              ,
                              P
                              )
                           
                         – a reference image 
                           O
                         and a corresponding list of locations 
                           P
                        . Similarly, 
                           
                              o
                              r
                              i
                              (
                              I
                              ,
                              t
                              )
                           
                         is the orientation on query image 
                           I
                         at location 
                           
                              t
                              ∈
                              R
                              (
                              c
                              +
                              r
                              )
                           
                         where 
                           
                              R
                              
                                 (
                                 c
                                 +
                                 r
                                 )
                              
                              =
                              
                                 [
                                 c
                                 +
                                 r
                                 −
                                 
                                    T
                                    2
                                 
                                 ,
                                 c
                                 +
                                 r
                                 +
                                 
                                    T
                                    2
                                 
                                 ]
                              
                              ×
                              
                                 [
                                 c
                                 +
                                 r
                                 −
                                 
                                    T
                                    2
                                 
                                 ,
                                 c
                                 +
                                 r
                                 +
                                 
                                    T
                                    2
                                 
                                 ]
                              
                           
                         defines a neighborhood of size T centered at the current position c on the query image shifted r.

Hinterstoisser et al. describe a computationally efficient way for the evaluation of the aforementioned similarity measure. In short, the orientations on the query image are quantized and encoded in a binary representation, each bit representing an individual quantized orientation. This way each pixel can contain multiple (or no) orientations. Quantized orientations are “spread” around their original positions by shifting the bitwise representation of the orientation over the 
                           R
                         neighborhood and merging all the shifted orientations with the bitwise OR operator. From the quantized and “spread” query image, n response maps are precomputed. Each response map contains an evaluated similarity measure (Eq. 1) where the orientation of template 
                           
                              o
                              r
                              i
                              (
                              O
                              ,
                              r
                              )
                           
                         is replaced with an orientation corresponding to each of the n bins. With the precomputed response maps, a template 
                           T
                         can be matched against the whole query image 
                           
                              I
                              ,
                           
                         simply by summing the (shifted) response maps.

We propose a modified
                         similarity measure:

                           
                              (2)
                              
                                 
                                    E
                                    
                                       (
                                       I
                                       ,
                                       T
                                       ,
                                       c
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          r
                                          ∈
                                          P
                                       
                                    
                                    
                                       max
                                       
                                          t
                                          ∈
                                          R
                                          (
                                          c
                                          +
                                          r
                                          )
                                       
                                    
                                    
                                       
                                          |
                                          cos
                                          
                                             (
                                             o
                                             r
                                             i
                                             
                                                (
                                                O
                                                ,
                                                r
                                                )
                                             
                                             −
                                             o
                                             r
                                             i
                                             
                                                (
                                                I
                                                ,
                                                t
                                                )
                                             
                                             )
                                          
                                          |
                                       
                                       p
                                    
                                    ,
                                 
                              
                           
                        where the additional exponent p adjusts the width of the error function (Fig. 2). When 
                           
                              p
                              =
                              0
                              ,
                           
                         the similarity measure quantifies the cross-section between the positions 
                           P
                         in template 
                           T
                         and the positions in query image 
                           I
                        . Whereas, when 
                           
                              p
                              =
                              ∞
                           
                         the similarity measure quantifies the cross-section between the positions in 
                           T
                         and positions in 
                           I
                         that have equal orientation. This modified similarity measure can be evaluated with a similar procedure as was proposed for the evaluation of the original similarity measure (Eq. 1); the procedure differs only at the calculation of the precomputed response maps.

To use template matching for pose estimation, a template needs to be generated for all relevant object poses. Templates can be generated either by capturing a real object from different views or by extracting the relevant information directly from a 3D CAD model. In the proposed method, the templates are generated directly from a 3D CAD model, which is usually readily available in industrial environments.

First, we generate a set of object views. The 3D CAD model is then transformed according to the view and finally the relevant information – edge and surface normal orientations – is extracted.

View directions are vectors from a point on a sphere to the center of the sphere; they are generated as described in what follows. Similar to [25], we begin with the mesh of the largest platonic solid – icosahedron. The mesh is subdivided using Loop subdivision [26], where the number of subdivisions depends on the number of views we want to generate (Fig. 3a). In the end, we remove the redundant views, e.g. half of the views can be removed if the object has an axis of symmetry (Fig. 3b). In the case of rotationally symmetric objects, view directions are simply vectors rotated along a single axis (Fig. 3c). Because we use a similarity measure that does not provide rotational invariance, we further generate a number of views uniformly rotated around each view’s direction.

Visible edges on the transformed reference model are determined in accord with either of the following criteria:

                              
                                 •
                                 the edge is visible if the angle between two visible adjoining faces is greater than ϕthr
                                    
                                 

the edge has only one visible adjoining face.

Points placed on the selected visible edges form a set of all possible positions E. Template 
                              T
                            is created by selecting a subset 
                              
                                 P
                                 ⊆
                                 E
                              
                            and their corresponding orientations in 
                              O
                           . The selected points 
                              P
                            should be spread uniformly over the object to improve robustness to partial occlusions of the object.

Templates for surface normal orientations are generated separately for the x- and y-axis. The template for each view consists of a set of positions 
                              P
                            and their corresponding surface normal orientations in 
                              O
                            (x- or y-axis), where positions 
                              P
                            are sampled uniformly on the visible surfaces of the transformed model.

Gradient orientation is a common form of object representation and has proven to be both discriminant and robust to illumination changes [27,28]. In addition, by wrapping the gradient orientation angles to [0, π), object representation becomes invariant to changes in the intensity of the background surrounding the object. Because we use surface normals obtained with photometric stereo that require multiple images of the same scene with different illuminations, multiple images are used to calculate gradient orientations as well. Gradient orientations are obtained by computing the gradient separately for each image and then for each location c selecting the gradient orientation of the image with the largest magnitude.

Gradient orientation is wrapped to interval [0, π) and quantized into n uniform bins. Similarly as in [25] we assign each location an orientation that occurs most frequently in its 3 × 3 neighborhood. To filter orientations due to noise we find the depth edges [22] and mask the gradient orientations elsewhere.

Surface normal orientations are used as an additional form of representation. To obtain surface normals we use a photometric stereo [29] that can produce a dense normal field at the very high level of detail, given an assumption of a Lambertian scene. The appearance I of a Lambertian scene observed under a lighting direction 
                              
                                 l
                                 ∈
                                 
                                    R
                                    3
                                 
                              
                            is described as:

                              
                                 (3)
                                 
                                    
                                       I
                                       =
                                       ρ
                                       n
                                       ·
                                       l
                                       ,
                                    
                                 
                              
                           where ρ is the diffuse albedo, and 
                              
                                 n
                                 ∈
                                 
                                    R
                                    3
                                 
                              
                            is the surface normal. To obtain surface normals n, at least three images acquired under different lighting directions l are required. Surface normal n is obtained by solving:

                              
                                 (4)
                                 
                                    
                                       G
                                       =
                                       
                                          
                                             (
                                             
                                                L
                                                T
                                             
                                             L
                                             )
                                          
                                          
                                             −
                                             1
                                          
                                       
                                       
                                          L
                                          T
                                       
                                       I
                                    
                                 
                              
                           and

                              
                                 (5)
                                 
                                    
                                       n
                                       =
                                       
                                          G
                                          
                                             ∥
                                             G
                                             ∥
                                          
                                       
                                    
                                 
                              
                           where L is a matrix of lighting directions, 
                              
                                 G
                                 =
                                 ρ
                                 n
                                 ,
                              
                            and I contains surface appearances for all lighting directions. Finally, surface normal orientations along x- and y-axis (Nx and Ny) are obtained with 
                              
                                 arctan
                                 
                                    
                                       n
                                       z
                                    
                                    
                                       n
                                       x
                                    
                                 
                              
                            and 
                              
                                 arctan
                                 
                                    
                                       n
                                       z
                                    
                                    
                                       n
                                       x
                                    
                                 
                                 ,
                              
                            respectively. Instead of splitting the normals into separate components Nx and Ny, we can also directly quantize each normal. One approach was proposed by Hinterstoisser et al. [25], where they set several precomputed vectors arranged in a circular cone shape originating from the peak of the cone pointing toward the camera. Each normal is then quantized by measuring the angles between the precomputed vectors.

The basic assumption of a perfect Lambertian scene is seldom observed with real images due to non-Lambertian surfaces, and the presence of shadows and specular reflections. Shadows arise from occlusions, self-occlusions and from surfaces facing away from the lighting direction. Irrespective of the type, all shadows on an image typically have low intensity; therefore, they can be detected by thresholding the intensity image [30]. At each detected location, we set the surface normal orientation to contain all possible values, i.e. we set all the bits in a binary representation of the quantized image. With that, we achieve that the similarity measure does not favor the poses that represent non-occluded objects. On the other hand, this can increase the number of false positives due to increased similarity measure values.

Specular reflections arise, when the object’s surface is not perfectly diffuse. Compared to shadows, they are much harder to detect and remove. Several methods [30–32] have been proposed for robust estimation of surface normals in the presence of specular reflections, shadows and non-Lambertian surfaces, but most require a large number (
                              
                                 6
                                 −
                                 20
                              
                           ) of illuminations to perform reliably. In our setup, specular reflections were avoided by using a cross polarization filter – a polarizing filter on both the light source and the camera.

All templates 
                           T
                         contain an equal number of elements (position-orientation pairs) in order to ensure predictable computational complexity and to be able to compare the similarity measures of different templates directly. The set of positions 
                           P
                         in each template 
                           T
                         is selected from the set of all possible positions E for the corresponding view, and |E| is a number of elements in E.

As already observed by Cai et al. [19], when |E| has high variance over the various poses that constitute a training set, template matching is biased toward simpler shapes (Fig. 4
                        ). To compensate for the bias toward simple shapes, we weight the evaluated similarity measure of each template 
                           
                              T
                              i
                           
                         with the corresponding weight Wi
                        :

                           
                              (6)
                              
                                 
                                    
                                       W
                                       i
                                    
                                    =
                                    
                                       
                                          (
                                          
                                             
                                                
                                                   |
                                                
                                                
                                                   E
                                                   i
                                                
                                                
                                                   |
                                                
                                             
                                             
                                                
                                                   max
                                                   i
                                                
                                                
                                                   
                                                      |
                                                   
                                                   
                                                      E
                                                      i
                                                   
                                                   
                                                      |
                                                   
                                                
                                             
                                          
                                          )
                                       
                                       b
                                    
                                    .
                                 
                              
                           
                        Setting b > 0 decreases the similarity measure for templates with |Ei
                        | < max 
                           i
                        |Ei
                        |. For 
                           
                              b
                              =
                              0
                           
                         the similarity measure corresponds to the similarity measure (Eq. 2).

The combined similarity measure is a product of the evaluated similarity measures for each modality multiplied with the weight Wi
                        .

                           
                              (7)
                              
                                 
                                    E
                                    
                                       (
                                       I
                                       ,
                                       
                                          T
                                          i
                                       
                                       ,
                                       c
                                       )
                                    
                                    =
                                    
                                       ∏
                                       
                                          ∀
                                          m
                                       
                                    
                                    E
                                    
                                       (
                                       
                                          I
                                          m
                                       
                                       ,
                                       
                                          T
                                          i
                                          m
                                       
                                       ,
                                       c
                                       )
                                    
                                    
                                       W
                                       i
                                       m
                                    
                                    ,
                                 
                              
                           
                        where 
                           
                              E
                              (
                              
                                 I
                                 m
                              
                              ,
                              
                                 T
                                 m
                              
                              ,
                              c
                              )
                           
                         denotes the similarity measure between query image 
                           
                              I
                              m
                           
                         and template 
                           
                              T
                              m
                           
                         at position c, and m denotes the modality. In our case, the modalities are surface normal orientations along the x- and y-axis and edge orientations.

We want to detect and estimate poses for multiple objects of the same type on a single image. Objects on each image are arbitrarily positioned and are variously occluded. We approach this problem by generating a number of possible pose hypotheses then iteratively selecting the best hypotheses.

With the procedure described in Section 3, an evaluated similarity map 
                           
                              E
                              (
                              I
                              ,
                              
                                 T
                                 i
                              
                              ,
                              c
                              )
                           
                         is obtained for each template 
                           
                              T
                              i
                           
                        . Let 
                           
                              
                                 {
                                 
                                    T
                                    i
                                 
                                 }
                              
                              ,
                              i
                              =
                              1
                              ,
                              …
                              ,
                              
                                 N
                                 t
                              
                           
                         denote templates generated from all view directions and 
                           
                              E
                              (
                              I
                              ,
                              
                                 T
                                 i
                              
                              ,
                              c
                              )
                           
                         denote an evaluated similarity measure map between query image 
                           I
                         and template 
                           
                              T
                              i
                           
                        .

First, all similarity measure maps 
                           
                              
                                 {
                                 E
                                 
                                    (
                                    I
                                    ,
                                    
                                       T
                                       i
                                    
                                    ,
                                    c
                                    )
                                 
                                 }
                              
                              ,
                              i
                              =
                              1
                              ,
                              …
                              ,
                              
                                 N
                                 t
                              
                           
                        , are combined together. The combined similarity measure map is calculated as:

                           
                              (8)
                              
                                 
                                    
                                       S
                                       ˜
                                    
                                    
                                       (
                                       c
                                       )
                                    
                                    =
                                    
                                       max
                                       i
                                    
                                    
                                       E
                                       (
                                       I
                                       ,
                                       
                                          T
                                          i
                                       
                                       ,
                                       c
                                       )
                                    
                                    ,
                                 
                              
                           
                        where c is the position on the similarity map. In addition to 
                           
                              
                                 S
                                 ˜
                              
                              ,
                           
                         we store the selected indices i for each location c. Stored indices correspond to the template with the maximum similarity measure at each location c.

On 
                           
                              S
                              ˜
                           
                         we detect clusters with the modified mean shift clustering algorithm proposed by Derganc et al. [33]. They modified the update step in the direction of the weighted mean of the density in the kernel with the step to the maximum value in the kernel:

                           
                              (9)
                              
                                 
                                    μ
                                    =
                                    arg
                                    
                                       max
                                       
                                          c
                                          ∈
                                          K
                                       
                                    
                                    
                                       S
                                       ˜
                                    
                                    
                                       (
                                       c
                                       )
                                    
                                    −
                                    c
                                 
                              
                           
                        where μ is the update step from the current position c, and K denotes the kernel. This modification improves the rate of convergence and gives the position of the maximum similarity measure of each detected mode. Each detected mode is a pose hypothesis and has a corresponding similarity score si
                        , a position and a template index i, effectively determining both the position and the rotation.

Hypotheses are selected in an iterative fashion. In each iteration, the pose hypothesis with the maximum similarity score smax
                         is chosen from the set of all hypotheses. The chosen hypothesis is again evaluated on the updated query image, using the same similarity measure as described in Section 3. Here we skip the optimizations proposed by Hinterstoisser et al. and simply iterate over all the points in the template 
                           T
                         re-evaluating the similarity score (2). If the re-evaluated similarity score differs from the smax
                        , the similarity score for the selected hypothesis is updated with the re-evaluated value and the iteration starts over. On the other hand, if the re-evaluated score equals smax
                         the hypothesis is selected and the query image is modified to account for the selected hypothesis.

The query image is modified to account for the selected hypothesis by masking the area corresponding to the currently selected pose hypothesis. On the edge orientation image, orientations are removed at edge positions of the object at the estimated pose by unsetting all the bits in the binary representation. However, the surface normal orientation image is not modified. The rationale for this is twofold. First, we observed that two occluded objects seldom share the same depth edges, thus removing depth edges in most cases affects only the hypotheses that describe the same object, which is not true for occluded surfaces. Second, because we combine similarity measures obtained from edges and surface normals with a multiplication, even changing one, has a significant effect on the resulting similarity measure.

The best hypotheses are iteratively selected until smax
                         is below a certain threshold or the maximum number of iterations is reached. Fig. 5 shows the whole hypotheses selection process.

@&#EXPERIMENTS AND RESULTS@&#

We conducted extensive evaluations of the proposed algorithm using challenging synthetic and real datasets. Additionally, we performed an evaluation of the proposed improvements to the similarity measure on a publicly available ACCV dataset of Hinterstoisser et al. [2]. For synthetic and real dataset we compared the proposed method with the methods proposed by Liu et al. [34] (FDCM) and by Hinterstoisser et al. [25]. On ACCV dataset, we additionally quantified the improvements to the similarity measure.

We quantitatively evaluated the proposed method on six synthetic datasets. Datasets were modeled with Blender and rendered with its Cycles renderer. Fig. 7 shows an example image from each dataset. From each synthetic scene, we extracted position and rotation for all of the objects on the scene, which served as ground truth. To account for the objects that are not visible or have most of their surface hidden, we calculated the percentage of occlusion for each object. The percentage of occlusion was obtained from the ratio between the rasterized area of the object in the scene and the rasterized area of the non-occluded object in the same pose. If the percentage of occlusion was above 70%, the object did not count as a false negative if not detected.

Pose estimation was additionally evaluated on several real datasets. For each dataset, a large number of objects of the same type were positioned in a cluttered manner as shown in Fig. 8. Datasets were captured with the setup displayed in Fig. 6a. The recording setup consisted of a single camera and three diffuse light sources. Both the light sources and the camera were equipped with a polarizing filter to remove specular reflections. The illumination directions were determined with a mirror-like sphere positioned in the target scene (Fig. 6b).

In order to
                        
                         evaluate the pose estimation reliability quantitatively, we needed to obtain ground truth for all of the objects. Since there was no simple way of obtaining this in a heavily cluttered environment, we manually annotated all the objects by hand.

@&#IMPLEMENTATION DETAILS@&#

The method was implemented in C++ and optimized for CPU execution. Some parts of the method could be further optimized with GPU implementation. For mesh manipulation, we used the open-source library OpenMesh [35]. Edge and surface normal orientations were quantized into eight bins. For objects with a general shape, we generated 162 views on a sphere; for rotationally symmetric objects, we generated views along the x-axis in 5° steps – generating 36 views. From each initially generated view, we further generated 36 views rotated around each view’s direction. For the modified mean shift clustering in the hypotheses generation phase we set the kernel size K at 5% of the object’s diameter. If not defined otherwise, we used all available modalities (E, Nx, Ny) and set b and p to 0.4 and 3, respectively. We selected parameter values 
                           
                              b
                              =
                              0.4
                           
                         and p = 3, by observing the performance on several manually selected sample images, which consisted of objects with high and with low variability of shape complexity among different views.

For the pose estimation method proposed by Liu et al. (FDCM), we used their publicly available source code. Object templates were generated for the same views as for our method. Poses were estimated iteratively in a similar fashion as described in Section 4. In each iteration, we chose the best hypothesis then removed its edges from the edge map. For FDCM we used only edge orientations, because to the best of our knowledge there is no simple way to incorporate normal information to it.

The method proposed by Hinterstoisser et al. was evaluated using the same source code that was used for the proposed method, but without the compensation for bias toward simple shapes (
                           
                              b
                              =
                              0
                           
                        ), the parametrization of the similarity measure (
                           
                              p
                              =
                              1
                           
                        ), and by combining the evaluated similarity measures of all modalities via summation instead of multiplication. In short, we evaluated only the improvement, due to the modifications to the similarity measure.

In order to evaluate the pose estimation on synthetic and real datasets we used the same metric as was used in [2]. With model 
                           
                              M
                              ,
                           
                         ground truth rotation R and translation T and the estimated rotation 
                           
                              R
                              ˜
                           
                         and translation 
                           
                              
                                 T
                                 ˜
                              
                              ,
                           
                         we computed the average distance of all the model points x from their transformed versions:

                           
                              (10)
                              
                                 
                                    e
                                    =
                                    
                                       avg
                                       
                                          x
                                          ∈
                                          M
                                       
                                    
                                    
                                       ∥
                                       
                                          (
                                          R
                                          x
                                          +
                                          T
                                          )
                                       
                                       −
                                       
                                          (
                                          
                                             R
                                             ˜
                                          
                                          x
                                          +
                                          
                                             T
                                             ˜
                                          
                                          )
                                       
                                       ∥
                                    
                                    .
                                 
                              
                           
                        For objects that could have ambiguous views, e.g. rotationally symmetric objects, the matching score was calculated as:

                           
                              (11)
                              
                                 
                                    e
                                    =
                                    
                                       avg
                                       
                                          
                                             x
                                             1
                                          
                                          ∈
                                          M
                                       
                                    
                                    
                                       min
                                       
                                          
                                             x
                                             2
                                          
                                          ∈
                                          M
                                       
                                    
                                    
                                       ∥
                                       
                                          (
                                          R
                                          
                                             x
                                             1
                                          
                                          +
                                          T
                                          )
                                       
                                       −
                                       
                                          (
                                          
                                             R
                                             ˜
                                          
                                          
                                             x
                                             2
                                          
                                          +
                                          
                                             T
                                             ˜
                                          
                                          )
                                       
                                       ∥
                                    
                                    .
                                 
                              
                           
                        In both cases, the detection was successful if the error measure e < kd, where d is a diameter of 
                           M
                         and k is a user determined constant factor. For both synthetic and real datasets we set 
                           
                              k
                              =
                              0.1
                           
                        .

@&#EVALUATION@&#


                           Fig. 9
                           
                            shows the detection rates of the compared methods for six synthetic datasets (Fig. 9a–f) and the combined detection rate for all synthetic datasets (Fig. 9g). On the first three datasets, both the proposed method and that by Hinterstoisser et al. performed similarly (Fig. 9a–c); nevertheless, the proposed method had a slightly higher detection rate due to the parametrization of the similarity measure and the way modalities were combined. In contrast, FDCM performed worst. This result is probably because the objects in datasets 1–3 had simple shapes with few edges, where edges alone are often insufficient for pose disambiguation, surface normals being necessary for reliable pose estimation.

The objects in datasets 4–6 were more complex and had a larger variance in the object’s shape complexity among different views. The proposed method performed best on all three datasets (Fig. 9d–f), whereas the method proposed by Hinterstoisser et al. performed the worst. This poor performance can be attributed to the large variance of the object’s size among different views. Without a compensation for a bias toward simple shapes, the estimated poses are often, and erroneously, the poses with the simplest shape. FDCM performed better than the method of Hinterstoisser et al., probably because FDCM also incorporates a model size bias and because the object shapes were more complex with larger number of edges.

The evaluation results for all real datasets are presented in Fig. 10. Additionally, Fig. 10f displays the combined detection rates for all real datasets. It shows that the proposed method performed best on average and on all individual datasets. Compared to the synthetic datasets the detection rates were lower on average due to errors in estimated normals, noise, and false edges.

The objects in first and second real datasets were similar to the objects in first and second synthetic datasets. Both datasets contained objects with simple shapes and few distinct edges. On both datasets, the method proposed by Hinterstoisser et al. performed much worse than on similarly shaped objects in synthetic datasets (Fig. 10a and b). FDCM similarly performed worse than the proposed method due to either ambiguous poses or false edges.

The shape of the objects in real dataset 3 directly corresponded to the shape of the objects in the synthetic dataset 3; however, the resulting detection rate (Fig. 10c) differed. As mentioned previously, several factors contribute to lower detection rates on real datasets. In addition, the objects in this real dataset had an imprint, which further contributed to false edges. FDCM achieved similar detection rates as the method proposed by Hinterstoisser et al. The results achieved on dataset 4 are similar (Fig. 10d).

On dataset 5, the
                            FDCM achieved a detection rate, very similar to that of the proposed method (Fig. 10e). The objects in dataset 5 had many distinct edge lines – something essential for FDCM’s search strategy.

Compensation for
                            bias toward simple shapes was evaluated for several values of b. Fig. 11 displays the combined detection rates for all synthetic datasets and all real datasets at different values of b. In addition, it shows the detection rates obtained for real datasets 1 and 5. For datasets with low variability of shape complexity among different views, e.g., dataset 1, compensation does not improve the detection rate (Fig. 11a). Conversely, the detection rate improves significantly for datasets with high variability of shape complexity, e.g., dataset 5 (Fig. 11b).

We further quantitatively evaluated detection rate using different modalities. The Fig. 12 presents the combined detection rates with respect to different input modalities for all synthetic and all real datasets. The results indicate that surface normal orientations alone are not sufficient for accurate detection and pose estimation, achieving a poor detection rate; however, surface normals can help to disambiguate among poses where edge information alone is insufficient. The results also show that even adding surface normal orientations along a single axis improves the detection rate. We evaluated both combining normals Nx and Ny into single discretized value (Nxy) as proposed by Hinterstoisser et al. and evaluating each normal component individually (Nx, Ny). The detection rates for both variants are similar.

The additional exponent p in the similarity measure (2) was quantitatively evaluated on synthetic and real datasets. Fig. 13 shows the combined detection rates for all synthetic and all real datasets at different values of p. When 
                              
                                 p
                                 =
                                 0
                              
                            the 
                              
                                 
                                    
                                       |
                                       cos
                                       
                                          (
                                          o
                                          r
                                          i
                                          
                                             (
                                             O
                                             ,
                                             r
                                             )
                                          
                                          −
                                          o
                                          r
                                          i
                                          
                                             (
                                             I
                                             ,
                                             t
                                             )
                                          
                                          )
                                       
                                       |
                                    
                                    p
                                 
                                 =
                                 1
                              
                            for all orientations 
                              
                                 o
                                 r
                                 i
                                 (
                                 O
                                 ,
                                 r
                                 )
                              
                            and 
                              
                                 o
                                 r
                                 i
                                 (
                                 I
                                 ,
                                 t
                                 )
                              
                           ; therefore, the similarity measure quantifies only the cross-section between positions in the template and valid positions on the query image, disregarding the orientation discrepancies altogether. On the other hand, by increasing p the width of the error function narrows and orientation error dominates the similarity measure. By using 
                              
                                 p
                                 =
                                 3
                              
                            the detection rate improved by ∼5% on average.

Furthermore, we quantitatively evaluated the suggested approach to combining similarity measures, i.e. multiplying instead of summing the similarity measures for all modalities. By multiplying the similarity measures, the detection rate is substantially improved on real datasets, where on the other hand the improvement on synthetic datasets is lower. Multiplication is important for (nearly) planar objects, which are common in industrial settings, because normal information alone leads to many false positive detections (Fig. 12 – results for Ny). By using multiplication, the detections are constrained by both edge and normal information.

Finally, we quantitatively evaluated iterative hypothesis selection. The proposed iterative selection strategy – with the query image update – was compared to a basic selection strategy. With basic strategy, final pose estimates were simply the detected clusters with highest similarity scores. Fig. 15
                            provides the combined detection rates for synthetic and real datasets for both selection strategies. By using iterative selection strategy, the detection rate is improved by ∼15% on both synthetic and real datasets.

To quantify the improvements to the similarity measure in a more general setting, we performed evaluation on the ACCV dataset of Hinterstoisser et al. [2]. The dataset contains color images and corresponding depth data for various objects. The evaluation was performed on selected objects (Fig. 16
                        ) for which a refined 3D model was available. In contrast to our dataset that was recorded with a multi-light setup, ACCV dataset consists only of single color images with corresponding depth maps; therefore, we cannot use the same method for extracting depth edges.

For each depth image D, we calculated depth gradient magnitude as 
                           
                              
                                 |
                                 ∇
                                 D
                                 |
                              
                              =
                              
                                 
                                    
                                       
                                          (
                                          
                                             
                                                ∂
                                                D
                                             
                                             
                                                ∂
                                                x
                                             
                                          
                                          )
                                       
                                       2
                                    
                                    +
                                    
                                       
                                          (
                                          
                                             
                                                ∂
                                                D
                                             
                                             
                                                ∂
                                                y
                                             
                                          
                                          )
                                       
                                       2
                                    
                                 
                              
                           
                        . Depth edges were then obtained by thresholding the |∇D|. Edges that occur due to invalid depth measurement were not filtered out or treated specially. Furthermore, we unset all the bits in a binary representation at the missing depth measurements. We empirically observed that setting the bits increased the number of false positive detections, due to many missing measurements. Fig. 17
                         shows edges obtained by thresholding the gradient on color image and by thresholding the depth gradient.

Training views were created according to the constraints set by Hinterstoisser et al. For training views we sampled only the upper half-sphere, created as described in Section 3.2, and limited in-plane rotations to an interval 
                           
                              [
                              −
                              
                                 45
                                 ∘
                              
                              ,
                              
                                 45
                                 ∘
                              
                              ]
                           
                         sampled with a 15° step. Distances from the objects were also limited to an interval [65 cm, 115 cm], which was sampled with a 15 cm step.

Similar to the dataset described above, we evaluated the contribution of each proposed modification. All evaluations unless stated otherwise were performed using 
                           
                              p
                              =
                              3
                              ,
                           
                        
                        
                           
                              b
                              =
                              0.4
                           
                         and using E, Nx, Ny modalities with multiplication for merging the modalities.

Compensation for bias was evaluated at several values of b. Fig. 18
                            shows the combined detection rates for all evaluated objects from the ACCV dataset. Fig. 19
                            shows a detected object (driller) without and with using compensation for bias. In this case, both poses accurately correspond to the edge and surface normal information; however, by compensating for bias toward simple shapes we estimate the correct pose.

Detection rates for several different modalities and combinations thereof are shown in Fig. 20
                           . The results are similar to the results obtained on synthetic and real dataset (Fig. 12). Detection rate, when separately evaluating each normal component is slightly higher than when combining both components into a single quantized value. However, separately evaluating each normal component requires ∼ 50% more time, which can be a significant limitation.

Evaluation of the similarity measure was also performed on ACCV dataset. Results (Fig. 21
                           ) show that increasing p, therefore narrowing the error function, improves the detection rate by ∼ 5%.

Furthermore, we evaluated detection rates when combining similarity measures by summation and multiplication. The results are shown in Fig. 22
                           . Average detection rates for summation and multiplication are similar, however performance for each differ for different object shapes.

@&#DISCUSSION@&#

Pose estimation of objects in cluttered environments is required for successful on-line analysis where pose constraints cannot be enforced by mechanical means. However, estimating the pose in unconstrained environments using only 2D information is problematic due to variable object appearance, changes in illumination, mechanical properties, and background clutter. Furthermore, with only edge orientation, disambiguation among different poses often becomes difficult (Fig. 23
                     ). Hinterstoisser et al. [25] demonstrated that by combining edge orientation with surface normal orientation, detection rate could be improved. In this paper, we suggested several improvements to their method, such as compensation for bias toward simple shapes, parameterization of the similarity measure, and different merging of different modalities. In addition, we used depth edges instead of color image gradients for detection, which eliminates the edges due to object’s texture. In addition, we proposed an iterative approach to hypotheses selection, which significantly improves the detection rate on scenes with multiple objects of the same type. We performed evaluation on datasets recorded using a multi-light setup, suitable for photometric stereo. Photometric stereo can produce a dense normal field at level of detail that cannot be achieved by any other triangulation-based approach. Additionally we evaluated improvements to the similarity measure on ACCV dataset of Hinterstoisser et al.

We compared the proposed method with the state-of-the-art method by Liu et al. [15] for pose estimation in industrial settings. Compared to it, we significantly improved the pose estimation reliability by combining multiple modalities. In addition to the edges obtained from an intensity image, we included surface normals obtained by photometric stereo. Where possible we obtained surface normal orientations along the x- and y-axis. For pose estimation in dynamic scenes, capturing several frames is often unfeasible; on the other hand, two sequential frames can usually be recorded in a short enough period with an overlapped exposure mode. With a two-light setup, only a projection of a single surface normal component can be obtained, which alone can significantly improve the pose estimation reliability (Fig. 12).

The proposed approach performed better than FDCM and original method proposed by Hinterstoisser et al. on all synthetic datasets (Fig. 9) and on all real datasets (Fig. 10). Most of the improvement in the detection rate can be attributed to the use of surface normal orientations as an additional form of scene representation. With only edge orientations, the difference in the detection rates between the proposed method and FDCM is much lower (Fig. 12). However, to the best of our knowledge, FDCM cannot be easily extended to combine multiple modalities.

Compared to the method proposed by Hinterstoisser et al. we improved the detection rate on synthetic and real datasets by merging the modalities with a multiplication instead of a summation (Fig. 14), incorporating compensation for bias toward simple shapes (Fig. 11), and parameterizing the similarity measure (Fig. 13). The proposed change to combine depth edges and surface normal orientations with a multiplication instead of a summation is suitable for (nearly) planar objects (Fig. 14), which are common in industrial settings, where normal information alone leads to many false positive detections (Fig. 12). By using multiplication, the detections are constrained by both edge and normal information. Additional evaluation on ACCV dataset of Hinterstoisser et al., showed that for complex surfaces, where surface normals alone can describe the object sufficiently well, summation performs similar to multiplication (Fig. 22). One downside of using multiplication is that the resulting similarity measure can be more sensitive to missing or erroneous measurement. Results obtained for compensation for bias toward simple shapes (Fig. 18) and parameterization of the similarity measure (Fig. 21) are similar to the results obtained on our datasets.

With photometric stereo we detected locations with unreliable normal estimations (shadowed areas) and set them as if they contain all possible orientations by setting all the bits in a binary representation; therefore, unreliable estimations did not contribute toward any pose in particular. Nevertheless, normal orientations can still contribute toward disambiguation of ambiguous object views (Fig. 12). This approach is only suitable for scenes with low amount of unreliable estimations; otherwise, the number of false positive detections can significantly increase due to high similarity measure values near unreliable normal estimations.

We showed that compensation for a bias toward simple shapes improves the detection rate for object shapes with high variability in |E| among different views (|E| being the number of visible edge points). Figs. 4 and 19 show several erroneous estimations due to this bias, where most of the estimates involved poses with the lowest |E|. Disambiguating between the estimated and the correct poses is impossible with edge and normal information only. We showed that the detection rate can be improved by introducing a compensation for the bias toward simple shapes (Figs. 11 and 18). There remains an open issue of finding the optimal value of the parameter used for compensation.

Hypothesis generation was performed on combined similarity measures due to the computational complexity of the modified mean shift algorithm. Finding an algorithm for more efficient cluster detection will be a part of future work. The detection of clusters on separate similarity maps would have several clear advantages, such as the detection of objects with the same center position but different poses.

Hypothesis selection followed a rather simple iterative procedure. The reason for iterative selection of the best candidate and masking the corresponding edge information lies in the fact that neighboring objects can affect each other’s estimated pose. We based the iterative procedure on the assumption that non-occluded objects have a higher similarity score; therefore, are selected first. The results indicate that by using an iterative selection strategy we can significantly improve the detection rate (Fig. 15).

@&#CONCLUSION@&#

In this paper, we proposed a method for real-time pose estimation of rigid objects in heavily cluttered environments. Using several synthetic and real datasets, we demonstrated that the method performs reliably. Compared to FDCM the detection rate is significantly improved. We show that combining the edge map with surface normal information improves the discriminative power of object templates. With respect to the method proposed by Hinterstoisser et al., we improved the pose estimation reliability for (nearly) planar objects, which are common in industrial settings, by compensating for bias toward simple shapes and by modifying the similarity measure.

The primary purpose of this research was to find a viable method for pose estimation that can be integrated into a computer vision system capable of real-time analysis of various dynamic processes; for example, counting defective products in a dynamic process, looking for surface variations among objects, or determining pose distribution. Future work will be directed in improving the pose estimation process and developing algorithms for tracking the aforementioned properties in a real-time process.

@&#ACKNOWLEDGMENTS@&#

This work was supported by the Ministry of Higher Education, Science and Technology, Republic of Slovenia under grants L2-4072, L2-5472, by Sensum, Computer Vision Systems, and by the European Union, European Social Fund.

@&#REFERENCES@&#

